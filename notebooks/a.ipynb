{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b91d224b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # True\n",
    "print(torch.cuda.get_device_name(0))  # NVIDIA GeForce RTX 4060"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c549de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Transformaciones para entrenamiento (aumentos + normalización)\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.05, 0.05)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Transformaciones para validación y test (solo reescalado + normalización)\n",
    "val_test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d9b3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# rutas (según tu estructura)\n",
    "# lo que tenías (mal): BASE_DIR = os.path.join(\"002cancer\")\n",
    "BASE_DIR = os.path.join(\"..\", \"002_cancer\")  # desde /notebooks sube a ../002_cancer\n",
    "TRAIN_DIR = os.path.join(BASE_DIR, \"train\")\n",
    "VAL_DIR   = os.path.join(BASE_DIR, \"val\")\n",
    "TEST_DIR  = os.path.join(BASE_DIR, \"test\")\n",
    "\n",
    "# parámetros que me pediste\n",
    "CLASSES = 2\n",
    "BATCH   = 32\n",
    "ROWS = COLS = 224\n",
    "INPUT_CH = 3\n",
    "EPOCHS  = 15\n",
    "TEST_MAX_SAMPLES = 2000\n",
    "SEED    = 42\n",
    "\n",
    "# dispositivo\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Usando dispositivo:\",DEVICE)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d9dd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset, DataLoader\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Comprobación de carpetas\n",
    "for p in [TRAIN_DIR, TEST_DIR]:\n",
    "    assert os.path.isdir(os.path.join(p, \"Benign\")),    f\"Falta {p}/Benign\"\n",
    "    assert os.path.isdir(os.path.join(p, \"Malignant\")), f\"Falta {p}/Malignant\"\n",
    "\n",
    "use_explicit_val = os.path.isdir(VAL_DIR) and all(\n",
    "    os.path.isdir(os.path.join(VAL_DIR, c)) for c in [\"Benign\", \"Malignant\"]\n",
    ")\n",
    "\n",
    "from torchvision import datasets\n",
    "if use_explicit_val:\n",
    "    train_ds = datasets.ImageFolder(TRAIN_DIR, transform=train_transforms)\n",
    "    val_ds   = datasets.ImageFolder(VAL_DIR,   transform=val_test_transforms)\n",
    "else:\n",
    "    full_train = datasets.ImageFolder(TRAIN_DIR, transform=train_transforms)\n",
    "    y = [lbl for _, lbl in full_train.samples]\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED)\n",
    "    train_idx, val_idx = next(sss.split(np.zeros(len(y)), y))\n",
    "    train_ds = Subset(full_train, train_idx);  train_ds.dataset.transform = train_transforms\n",
    "    val_ds   = Subset(full_train, val_idx);    val_ds.dataset.transform   = val_test_transforms\n",
    "\n",
    "test_ds  = datasets.ImageFolder(TEST_DIR, transform=val_test_transforms)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "PIN = (DEVICE.type == \"cuda\")\n",
    "NUM_WORKERS = 0 if os.name == \"nt\" else 4\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    import numpy as _np, random as _rand\n",
    "    _np.random.seed(SEED + worker_id); _rand.seed(SEED + worker_id)\n",
    "\n",
    "g = torch.Generator(); g.manual_seed(SEED)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=PIN,\n",
    "                          worker_init_fn=seed_worker, generator=g)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=PIN,\n",
    "                          worker_init_fn=seed_worker)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=PIN,\n",
    "                          worker_init_fn=seed_worker)\n",
    "\n",
    "# Mapeo de clases\n",
    "if use_explicit_val:\n",
    "    class_to_idx = datasets.ImageFolder(TRAIN_DIR).class_to_idx\n",
    "else:\n",
    "    class_to_idx = train_ds.dataset.class_to_idx if hasattr(train_ds, \"dataset\") else datasets.ImageFolder(TRAIN_DIR).class_to_idx\n",
    "idx_to_class = {v:k for k,v in class_to_idx.items()}\n",
    "print(\"Clases:\", class_to_idx)\n",
    "print(f\"Train={len(train_ds)} | Val={len(val_ds)} | Test={len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fe4deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Modelo estable con GroupNorm =====\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "def _gn_groups(ch):\n",
    "    # 32 es estándar; si no divide, baja a 16/8/4/1\n",
    "    for g in [32,16,8,4,1]:\n",
    "        if ch % g == 0: return g\n",
    "    return 1\n",
    "\n",
    "class ConvGNReLU(nn.Sequential):\n",
    "    def __init__(self, in_ch, out_ch, k=3, s=1, p=1):\n",
    "        super().__init__(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=k, stride=s, padding=p, bias=False),\n",
    "            nn.GroupNorm(_gn_groups(out_ch), out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "class StableCNN(nn.Module):\n",
    "    def __init__(self, in_ch=3, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            ConvGNReLU(in_ch,   64),\n",
    "            ConvGNReLU(64,      64),\n",
    "            nn.MaxPool2d(2),          # 224 -> 112\n",
    "\n",
    "            ConvGNReLU(64,     128),\n",
    "            ConvGNReLU(128,    128),\n",
    "            nn.MaxPool2d(2),          # 112 -> 56\n",
    "\n",
    "            ConvGNReLU(128,    256),\n",
    "            ConvGNReLU(256,    256),\n",
    "            nn.MaxPool2d(2),          # 56 -> 28\n",
    "\n",
    "            ConvGNReLU(256,    256),\n",
    "            nn.MaxPool2d(2),          # 28 -> 14\n",
    "        )\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)  # robusto a tamaño\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.30),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.30),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        self._init()\n",
    "\n",
    "    def _init(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(m.weight, a=math.sqrt(5))\n",
    "                if m.bias is not None: nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "model = StableCNN(in_ch=INPUT_CH, num_classes=CLASSES).to(DEVICE)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8719c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "# Label Smoothing (reduce picos/varianza)\n",
    "class LabelSmoothingCE(nn.Module):\n",
    "    def __init__(self, eps=0.05):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "    def forward(self, logits, target):\n",
    "        n_classes = logits.size(1)\n",
    "        logprobs  = self.log_softmax(logits)\n",
    "        nll = nn.functional.nll_loss(logprobs, target, reduction='mean')\n",
    "        smooth = -logprobs.mean(dim=1).mean()\n",
    "        return (1 - self.eps) * nll + self.eps * smooth\n",
    "\n",
    "criterion = LabelSmoothingCE(eps=0.05)\n",
    "\n",
    "# LR más bajo + weight decay más fuerte\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=5e-4)\n",
    "\n",
    "# Warmup + Cosine (más suave que ReduceLROnPlateau)\n",
    "WARMUP_EPOCHS = 3\n",
    "scheduler_cos = CosineAnnealingLR(optimizer, T_max=max(1, EPOCHS - WARMUP_EPOCHS), eta_min=1e-6)\n",
    "\n",
    "USE_AMP = (DEVICE.type == \"cuda\")\n",
    "scaler  = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
    "\n",
    "def scheduler_step(epoch):\n",
    "    if epoch < WARMUP_EPOCHS:\n",
    "        for pg in optimizer.param_groups:\n",
    "            pg['lr'] = 3e-4 * (epoch + 1) / WARMUP_EPOCHS\n",
    "    else:\n",
    "        scheduler_cos.step()\n",
    "\n",
    "def run_epoch(loader, train_mode=True):\n",
    "    model.train() if train_mode else model.eval()\n",
    "    total, correct, running = 0, 0, 0.0\n",
    "\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        if train_mode:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if USE_AMP:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                logits = model(xb)\n",
    "                loss   = criterion(logits, yb)\n",
    "        else:\n",
    "            logits = model(xb); loss = criterion(logits, yb)\n",
    "\n",
    "        if train_mode:\n",
    "            if USE_AMP:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer); scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "        running += loss.item() * xb.size(0)\n",
    "        pred = logits.argmax(1)\n",
    "        total += yb.size(0)\n",
    "        correct += (pred == yb).sum().item()\n",
    "\n",
    "    return running/total, correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81efb157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento completo (sin early stopping) guardando histórico\n",
    "import numpy as np, os\n",
    "\n",
    "history = {\n",
    "    \"EPOCHS\": [],\n",
    "    \"train_loss\": [], \"val_loss\": [],\n",
    "    \"train_acc\":  [], \"val_acc\":  [],\n",
    "}\n",
    "\n",
    "# Si ya tienes class_to_idx, omite esto.\n",
    "try:\n",
    "    class_to_idx\n",
    "except NameError:\n",
    "    from torchvision import datasets\n",
    "    class_to_idx = datasets.ImageFolder(TRAIN_DIR).class_to_idx\n",
    "\n",
    "idx_to_class = {v:k for k,v in class_to_idx.items()}\n",
    "malignant_index = class_to_idx.get(\"Malignant\", 1)  # por si la carpeta se llama distinto, ajusta aquí\n",
    "print(\"class_to_idx:\", class_to_idx, \"| malignant_index:\", malignant_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cc407e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_with_fn(loader):\n",
    "    model.eval()\n",
    "    total, correct, running = 0, 0, 0.0\n",
    "    fn_total = 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "\n",
    "        if USE_AMP:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                logits = model(xb)\n",
    "                loss   = criterion(logits, yb)\n",
    "        else:\n",
    "            logits = model(xb)\n",
    "            loss   = criterion(logits, yb)\n",
    "\n",
    "        running += loss.item() * xb.size(0)\n",
    "        preds = logits.argmax(1)\n",
    "\n",
    "        total += yb.size(0)\n",
    "        correct += (preds == yb).sum().item()\n",
    "\n",
    "        # FN = verdaderos Malignant predichos como NO-Malignant\n",
    "        fn_total += ((preds != malignant_index) & (yb == malignant_index)).sum().item()\n",
    "\n",
    "    return running/total, correct/total, fn_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533b6e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "best_val_fn   = float(\"inf\")\n",
    "best_path     = \"stablecnn_minFN.pt\"\n",
    "# Si NO quieres early stopping, pon patience=None\n",
    "patience      = None\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    tr_loss, tr_acc = run_epoch(train_loader, train_mode=True)\n",
    "    va_loss, va_acc, va_fn = evaluate_with_fn(val_loader)\n",
    "\n",
    "    # scheduler suave\n",
    "    scheduler_step(epoch)\n",
    "\n",
    "    # histórico\n",
    "    history[\"EPOCHS\"].append(epoch)\n",
    "    history[\"train_loss\"].append(tr_loss); history[\"train_acc\"].append(tr_acc)\n",
    "    history[\"val_loss\"].append(va_loss);   history[\"val_acc\"].append(va_acc)\n",
    "\n",
    "    print(f\"[{epoch:02d}/{EPOCHS}] \"\n",
    "          f\"train_loss={tr_loss:.4f} acc={tr_acc:.3f} | \"\n",
    "          f\"val_loss={va_loss:.4f} acc={va_acc:.3f} | \"\n",
    "          f\"val_FN={va_fn} | \"\n",
    "          f\"lr={optimizer.param_groups[0]['lr']:.6f}\", flush=True)\n",
    "\n",
    "    # guardar mejor por FN (desempata por menor val_loss)\n",
    "    improved = (va_fn < best_val_fn) or ((va_fn == best_val_fn) and (va_loss < best_val_loss - 1e-4))\n",
    "    if improved:\n",
    "        best_val_fn = va_fn; best_val_loss = va_loss; epochs_no_improve = 0\n",
    "        torch.save({\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"class_to_idx\": class_to_idx,\n",
    "            \"malignant_index\": malignant_index,\n",
    "            \"best_val_fn\": best_val_fn,\n",
    "            \"best_val_loss\": best_val_loss,\n",
    "            \"epoch\": epoch\n",
    "        }, best_path)\n",
    "        print(f\"✔️ Guardado mejor modelo (min FN): FN={best_val_fn}, loss={best_val_loss:.4f} -> {best_path}\", flush=True)\n",
    "    else:\n",
    "        if patience is not None:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(\"Early stopping (sin mejora en FN).\", flush=True)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9eab0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(\"stablecnn_minFN.pt\", map_location=DEVICE)\n",
    "model.load_state_dict(ckpt[\"model_state\"])\n",
    "class_to_idx = ckpt[\"class_to_idx\"]\n",
    "malignant_index = ckpt.get(\"malignant_index\", class_to_idx.get(\"Malignant\", 1))\n",
    "idx_to_class = {v:k for k,v in class_to_idx.items()}\n",
    "print(f\"Mejor modelo cargado | FN={ckpt.get('best_val_fn')} loss={ckpt.get('best_val_loss')}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9218c959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def smooth(x, k=3):\n",
    "    import numpy as np\n",
    "    if len(x) < k: return np.array(x, float)\n",
    "    x = np.array(x, float)\n",
    "    return np.convolve(x, np.ones(k)/k, mode=\"valid\")\n",
    "\n",
    "k = 3\n",
    "if len(history[\"EPOCHS\"]) >= k:\n",
    "    e2 = history[\"EPOCHS\"][k-1:]\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(e2, smooth(history[\"train_loss\"],k), label=\"Train loss (sm)\")\n",
    "    plt.plot(e2, smooth(history[\"val_loss\"],k), label=\"Val loss (sm)\")\n",
    "    plt.xlabel(\"Época\"); plt.ylabel(\"Pérdida\"); plt.title(\"Loss suavizada\")\n",
    "    plt.legend(); plt.grid(True); plt.show()\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(e2, smooth(history[\"train_acc\"],k), label=\"Train acc (sm)\")\n",
    "    plt.plot(e2, smooth(history[\"val_acc\"],k), label=\"Val acc (sm)\")\n",
    "    plt.xlabel(\"Época\"); plt.ylabel(\"Exactitud\"); plt.title(\"Accuracy suavizada\")\n",
    "    plt.legend(); plt.grid(True); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95374c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        if USE_AMP:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                logits = model(xb)\n",
    "        else:\n",
    "            logits = model(xb)\n",
    "        pred = logits.argmax(1).cpu()\n",
    "        all_preds.append(pred)\n",
    "        all_labels.append(yb)\n",
    "\n",
    "all_preds  = torch.cat(all_preds).numpy()\n",
    "all_labels = torch.cat(all_labels).numpy()\n",
    "\n",
    "test_acc = (all_preds == all_labels).mean()\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "print(classification_report(all_labels, all_preds,\n",
    "                            target_names=[idx_to_class[0], idx_to_class[1]]))\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds, labels=[0,1])\n",
    "\n",
    "plt.figure(figsize=(4.8,4.8))\n",
    "im = plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "plt.title(\"Matriz de confusión\")\n",
    "tick_marks = np.arange(2)\n",
    "plt.xticks(tick_marks, [idx_to_class[0], idx_to_class[1]], rotation=45, ha=\"right\")\n",
    "plt.yticks(tick_marks, [idx_to_class[0], idx_to_class[1]])\n",
    "plt.xlabel(\"Predicción\"); plt.ylabel(\"Real\")\n",
    "\n",
    "th = cm.max()/2.0 if cm.size > 0 else 0.0\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, format(cm[i, j], \"d\"),\n",
    "                 ha=\"center\", va=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > th else \"black\")\n",
    "\n",
    "plt.tight_layout(); plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
